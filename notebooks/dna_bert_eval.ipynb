{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3bc9794c-c682-4f51-8295-36477b6c6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/s/project/denovo-prosit/JohannesHingerl/BERTADN/outputs_BERTADN_32GPU/checkpoint-200000/\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"/s/project/denovo-prosit/JohannesHingerl/BERTADN/outputs_BERTADN_32GPU/checkpoint-20000/\")\n",
    "\n",
    "\n",
    "import torch \n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import math\n",
    "import itertools\n",
    "from collections.abc import Mapping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a16e7-d43d-4817-83e9-09842ada9262",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32485ee-d112-4d0b-8d02-34049891cdaf",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fa321a1c-e4a8-4efb-8bd0-65588c985d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkstring(string, length):\n",
    "    # chunks a string into segments of length\n",
    "    return (string[0+i:length+i] for i in range(0, len(string), length))\n",
    "\n",
    "def kmers(seq, k=6):\n",
    "    # splits a sequence into non-overlappnig k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq), k) if i + k <= len(seq)]\n",
    "\n",
    "def kmers_stride1(seq, k=6):\n",
    "    # splits a sequence into overlapping k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq)-k+1)]   \n",
    "\n",
    "def tok_func(x): return tokenizer(\" \".join(kmers_stride1(x[\"seq_chunked\"])))\n",
    "\n",
    "def one_hot_encode(gts, dim=5):\n",
    "    result = []\n",
    "    nuc_dict = {\"A\":0,\"C\":1,\"G\":2,\"T\":3}\n",
    "    for nt in gts:\n",
    "        vec = np.zeros(dim)\n",
    "        vec[nuc_dict[nt]] = 1\n",
    "        result.append(vec)\n",
    "    return np.stack(result, axis=0)\n",
    "\n",
    "def class_label_gts(gts):\n",
    "    nuc_dict = {\"A\":0,\"C\":1,\"G\":2,\"T\":3}\n",
    "    return np.array([nuc_dict[x] for x in gts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d63eb49f-0ab2-4f0c-8557-f476c4485ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import  DataCollatorForLanguageModeling\n",
    "#data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=True, mlm_probability = 0.15)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def _torch_collate_batch(examples, tokenizer, pad_to_multiple_of = None):\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "    import torch\n",
    "\n",
    "    # Tensorize if necessary.\n",
    "    if isinstance(examples[0], (list, tuple, np.ndarray)):\n",
    "        examples = [torch.tensor(e, dtype=torch.long) for e in examples]\n",
    "\n",
    "    length_of_first = examples[0].size(0)\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "\n",
    "    are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "        return torch.stack(examples, dim=0)\n",
    "\n",
    "    # If yes, check if we have a `pad_token`.\n",
    "    if tokenizer._pad_token is None:\n",
    "        raise ValueError(\n",
    "            \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "        )\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    max_length = max(x.size(0) for x in examples)\n",
    "    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "    result = examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)\n",
    "    for i, example in enumerate(examples):\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            result[i, : example.shape[0]] = example\n",
    "        else:\n",
    "            result[i, -example.shape[0] :] = example\n",
    "    return result\n",
    "\n",
    "class DataCollatorForLanguageModelingSpan():\n",
    "    def __init__(self, tokenizer, mlm, mlm_probability, span_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mlm = mlm\n",
    "        self.span_length =span_length\n",
    "        self.mlm_probability= mlm_probability\n",
    "        self.pad_to_multiple_of = span_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\n",
    "                \"input_ids\": _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "            }\n",
    "\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        if self.mlm:\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.torch_mask_tokens(\n",
    "                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "            )\n",
    "        else:\n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    def torch_mask_tokens(self, inputs, special_tokens_mask):\n",
    "        import torch\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability*0.2)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool().numpy()\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * self.span_length, mode = 'same' ),axis = 1, arr = masked_indices).astype(bool) \n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        m_save = masked_indices.clone()\n",
    "        \n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability*0.8) \n",
    "        probability_matrix.masked_fill_(masked_indices, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool().numpy()\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * self.span_length, mode = 'same' ),axis = 1, arr = masked_indices).astype(bool) \n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        m_final = masked_indices + m_save \n",
    "        labels[~m_final] = -100  # We only compute loss on masked tokens\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        #indices_replaced = torch.bernoulli(torch.full(labels.shape, 1.0)).bool()\n",
    "        #print (indices_replaced)\n",
    "        inputs[masked_indices] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        #print (masked_indices)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        #indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        #random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        #inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b1cbc-11ab-4355-acb8-41a92d826060",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "80d3c3a7-f9d8-4d48-aa48-19a1fca8c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_batch(tokenized_data, dataset, seq_idx):\n",
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    label = dataset.iloc[seq_idx]['UTR3_seq']\n",
    "    label_len = len(label)\n",
    "    if label_len < 6:\n",
    "        return torch.zeros(label_len,label_len,5)\n",
    "    else:\n",
    "        diag_matrix = torch.eye(tokenized_data['input_ids'].shape[1]).numpy()\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * 6, mode = 'same' ),axis = 1, arr = diag_matrix).astype(bool)\n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        masked_indices = masked_indices[3:label_len-5-2]\n",
    "        res = tokenized_data['input_ids'].expand(masked_indices.shape[0],-1).clone()\n",
    "        res[masked_indices] = 4\n",
    "        #print (res[0], res.shape)\n",
    "        res = res.to(device)\n",
    "        with torch.no_grad():\n",
    "            fin_calculation = torch.softmax(model(res)['logits'], dim=2).detach().cpu()   \n",
    "        return fin_calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b9fe45-2c83-445f-b990-1f0dc8f7efbb",
   "metadata": {},
   "source": [
    "## Translating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3a93927f-c4c5-4f4f-8452-5cbfdb022e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prbs_from_pred(prediction, pred_pos, token_pos, label_pos, label):   \n",
    "    # pred_pos = \"kmer\" position in tokenized sequence (incl. special tokens)\n",
    "    # token_pos = position of nucleotide in kmer\n",
    "    # label_pos = position of actual nucleotide in sequence\n",
    "    model_pred = prediction\n",
    "    prbs = [torch.sum(model_pred[pred_pos,tokendict_list[token_pos][nuc]]) for nuc in [\"A\",\"C\",\"G\",\"T\"]]\n",
    "    gt = label[label_pos] # 6-CLS, zerobased\n",
    "    res = torch.tensor(prbs+[0.0])\n",
    "    return res, gt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca263894-392a-44f5-bb47-f0ec30eebbe9",
   "metadata": {},
   "source": [
    "# Prepare inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3a469f-dc86-465a-81bc-3a7923631f3c",
   "metadata": {},
   "source": [
    "## Prepare dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5257c3d6-b7bb-4520-a13a-4c1ffc6c6037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene</th>\n",
       "      <th>chr</th>\n",
       "      <th>strand</th>\n",
       "      <th>UTR3_start</th>\n",
       "      <th>UTR3_end</th>\n",
       "      <th>binding_start</th>\n",
       "      <th>binding_end</th>\n",
       "      <th>sequence</th>\n",
       "      <th>original</th>\n",
       "      <th>seq_range</th>\n",
       "      <th>gpar_binding</th>\n",
       "      <th>UTR3_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YAL003W</td>\n",
       "      <td>I</td>\n",
       "      <td>+</td>\n",
       "      <td>143161</td>\n",
       "      <td>143381</td>\n",
       "      <td>152</td>\n",
       "      <td>173</td>\n",
       "      <td>AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...</td>\n",
       "      <td>True</td>\n",
       "      <td>(0, 221)</td>\n",
       "      <td>(152, 173)</td>\n",
       "      <td>AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YAL003W</td>\n",
       "      <td>I</td>\n",
       "      <td>+</td>\n",
       "      <td>143161</td>\n",
       "      <td>143381</td>\n",
       "      <td>152</td>\n",
       "      <td>173</td>\n",
       "      <td>AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...</td>\n",
       "      <td>False</td>\n",
       "      <td>(221, 442)</td>\n",
       "      <td>(373, 394)</td>\n",
       "      <td>AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YAL003W</td>\n",
       "      <td>I</td>\n",
       "      <td>+</td>\n",
       "      <td>143161</td>\n",
       "      <td>143381</td>\n",
       "      <td>152</td>\n",
       "      <td>173</td>\n",
       "      <td>AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...</td>\n",
       "      <td>False</td>\n",
       "      <td>(442, 663)</td>\n",
       "      <td>(594, 615)</td>\n",
       "      <td>AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YAL003W</td>\n",
       "      <td>I</td>\n",
       "      <td>+</td>\n",
       "      <td>143161</td>\n",
       "      <td>143381</td>\n",
       "      <td>152</td>\n",
       "      <td>173</td>\n",
       "      <td>AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...</td>\n",
       "      <td>False</td>\n",
       "      <td>(663, 884)</td>\n",
       "      <td>(815, 836)</td>\n",
       "      <td>AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YAL003W</td>\n",
       "      <td>I</td>\n",
       "      <td>+</td>\n",
       "      <td>143161</td>\n",
       "      <td>143381</td>\n",
       "      <td>152</td>\n",
       "      <td>173</td>\n",
       "      <td>AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...</td>\n",
       "      <td>False</td>\n",
       "      <td>(884, 1105)</td>\n",
       "      <td>(1036, 1057)</td>\n",
       "      <td>AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27119</th>\n",
       "      <td>YPR199C</td>\n",
       "      <td>XVI</td>\n",
       "      <td>-</td>\n",
       "      <td>937969</td>\n",
       "      <td>938147</td>\n",
       "      <td>31</td>\n",
       "      <td>55</td>\n",
       "      <td>TATTCTTCCTGTAAGCACAGCGCAACATTGTCTTATTCTTAATATT...</td>\n",
       "      <td>False</td>\n",
       "      <td>(5022665, 5022844)</td>\n",
       "      <td>(5022696, 5022720)</td>\n",
       "      <td>TATTCTTCCTGTAAGCACAGCGCAACATTGTCTTATTCTTAATATT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27120</th>\n",
       "      <td>YPR199C</td>\n",
       "      <td>XVI</td>\n",
       "      <td>-</td>\n",
       "      <td>937969</td>\n",
       "      <td>938147</td>\n",
       "      <td>31</td>\n",
       "      <td>55</td>\n",
       "      <td>TATTCTTCCTGTAAGCACAGCGCAACATTGTTGTATCTCTTTAAGT...</td>\n",
       "      <td>False</td>\n",
       "      <td>(5022844, 5023023)</td>\n",
       "      <td>(5022875, 5022899)</td>\n",
       "      <td>TATTCTTCCTGTAAGCACAGCGCAACATTGTTGTATCTCTTTAAGT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27121</th>\n",
       "      <td>YPR199C</td>\n",
       "      <td>XVI</td>\n",
       "      <td>-</td>\n",
       "      <td>937969</td>\n",
       "      <td>938147</td>\n",
       "      <td>31</td>\n",
       "      <td>55</td>\n",
       "      <td>TATTCTTCCTGTAAGCACAGCGCAACATTGTTAGTAAATTTCTCTA...</td>\n",
       "      <td>False</td>\n",
       "      <td>(5023023, 5023202)</td>\n",
       "      <td>(5023054, 5023078)</td>\n",
       "      <td>TATTCTTCCTGTAAGCACAGCGCAACATTGTTAGTAAATTTCTCTA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27122</th>\n",
       "      <td>YPR199C</td>\n",
       "      <td>XVI</td>\n",
       "      <td>-</td>\n",
       "      <td>937969</td>\n",
       "      <td>938147</td>\n",
       "      <td>31</td>\n",
       "      <td>55</td>\n",
       "      <td>TATTCTTCCTGTAAGCACAGCGCAACATTGTATGTATTTATCTTTA...</td>\n",
       "      <td>False</td>\n",
       "      <td>(5023202, 5023381)</td>\n",
       "      <td>(5023233, 5023257)</td>\n",
       "      <td>TATTCTTCCTGTAAGCACAGCGCAACATTGTATGTATTTATCTTTA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27123</th>\n",
       "      <td>YPR199C</td>\n",
       "      <td>XVI</td>\n",
       "      <td>-</td>\n",
       "      <td>937969</td>\n",
       "      <td>938147</td>\n",
       "      <td>31</td>\n",
       "      <td>55</td>\n",
       "      <td>TATTCTTCCTGTAAGCACAGCGCAACATTGTTATTCTTTGTAATAT...</td>\n",
       "      <td>False</td>\n",
       "      <td>(5023381, 5023560)</td>\n",
       "      <td>(5023412, 5023436)</td>\n",
       "      <td>TATTCTTCCTGTAAGCACAGCGCAACATTGTTATTCTTTGTAATAT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27124 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          gene  chr strand  UTR3_start  UTR3_end  binding_start  binding_end  \\\n",
       "0      YAL003W    I      +      143161    143381            152          173   \n",
       "1      YAL003W    I      +      143161    143381            152          173   \n",
       "2      YAL003W    I      +      143161    143381            152          173   \n",
       "3      YAL003W    I      +      143161    143381            152          173   \n",
       "4      YAL003W    I      +      143161    143381            152          173   \n",
       "...        ...  ...    ...         ...       ...            ...          ...   \n",
       "27119  YPR199C  XVI      -      937969    938147             31           55   \n",
       "27120  YPR199C  XVI      -      937969    938147             31           55   \n",
       "27121  YPR199C  XVI      -      937969    938147             31           55   \n",
       "27122  YPR199C  XVI      -      937969    938147             31           55   \n",
       "27123  YPR199C  XVI      -      937969    938147             31           55   \n",
       "\n",
       "                                                sequence  original  \\\n",
       "0      AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...      True   \n",
       "1      AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...     False   \n",
       "2      AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...     False   \n",
       "3      AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...     False   \n",
       "4      AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...     False   \n",
       "...                                                  ...       ...   \n",
       "27119  TATTCTTCCTGTAAGCACAGCGCAACATTGTCTTATTCTTAATATT...     False   \n",
       "27120  TATTCTTCCTGTAAGCACAGCGCAACATTGTTGTATCTCTTTAAGT...     False   \n",
       "27121  TATTCTTCCTGTAAGCACAGCGCAACATTGTTAGTAAATTTCTCTA...     False   \n",
       "27122  TATTCTTCCTGTAAGCACAGCGCAACATTGTATGTATTTATCTTTA...     False   \n",
       "27123  TATTCTTCCTGTAAGCACAGCGCAACATTGTTATTCTTTGTAATAT...     False   \n",
       "\n",
       "                seq_range        gpar_binding  \\\n",
       "0                (0, 221)          (152, 173)   \n",
       "1              (221, 442)          (373, 394)   \n",
       "2              (442, 663)          (594, 615)   \n",
       "3              (663, 884)          (815, 836)   \n",
       "4             (884, 1105)        (1036, 1057)   \n",
       "...                   ...                 ...   \n",
       "27119  (5022665, 5022844)  (5022696, 5022720)   \n",
       "27120  (5022844, 5023023)  (5022875, 5022899)   \n",
       "27121  (5023023, 5023202)  (5023054, 5023078)   \n",
       "27122  (5023202, 5023381)  (5023233, 5023257)   \n",
       "27123  (5023381, 5023560)  (5023412, 5023436)   \n",
       "\n",
       "                                                UTR3_seq  \n",
       "0      AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...  \n",
       "1      AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...  \n",
       "2      AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...  \n",
       "3      AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...  \n",
       "4      AAGGCTTTTTTATAAACTTTTTATAATTAACATTAAAGCAAAAACA...  \n",
       "...                                                  ...  \n",
       "27119  TATTCTTCCTGTAAGCACAGCGCAACATTGTCTTATTCTTAATATT...  \n",
       "27120  TATTCTTCCTGTAAGCACAGCGCAACATTGTTGTATCTCTTTAAGT...  \n",
       "27121  TATTCTTCCTGTAAGCACAGCGCAACATTGTTAGTAAATTTCTCTA...  \n",
       "27122  TATTCTTCCTGTAAGCACAGCGCAACATTGTATGTATTTATCTTTA...  \n",
       "27123  TATTCTTCCTGTAAGCACAGCGCAACATTGTTATTCTTTGTAATAT...  \n",
       "\n",
       "[27124 rows x 12 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = pd.read_csv(\"/s/project/semi_supervised_multispecies/all_fungi_reference/fungi/Annotation/Sequences/AAA_Concatenated/Scer_half_life.csv\")\n",
    "dataset = pd.read_csv(\"/s/project/semi_supervised_multispecies/Downstream/gPAR_CLIP/gpar_clip_downstream.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "230e0d37-9326-4db7-8a16-76afc6247be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5b5499e48344baa170b8a4f6d6c940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/15485 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce15219931ed48338897675ddbf3806d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/15485 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['seq_len'] = dataset['UTR3_seq'].apply(lambda x: len(x))\n",
    "\n",
    "dataset['seq_chunked'] = dataset['UTR3_seq'].apply(lambda x : list(chunkstring(x, 300)))\n",
    "dataset = dataset.explode('seq_chunked').reset_index()\n",
    "ds = Dataset.from_pandas(dataset[['seq_chunked']])\n",
    "\n",
    "tok_ds = ds.map(tok_func, batched=False,  num_proc=2)\n",
    "\n",
    "rem_tok_ds = tok_ds.remove_columns('seq_chunked')\n",
    "\n",
    "data_collator = DataCollatorForLanguageModelingSpan(tokenizer, mlm=False, mlm_probability = 0.025, span_length =6)\n",
    "data_loader = torch.utils.data.DataLoader(rem_tok_ds, batch_size=1, collate_fn=data_collator, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed132ad6-0aad-4cab-8dde-919719c02abc",
   "metadata": {},
   "source": [
    "## Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04f98016-880a-43c7-993c-35fe8fd5b69b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44910007-d73d-4499-83b3-2fec77d686be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "computed = []\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd982373-8c17-46c0-abed-7e1c72df1eed",
   "metadata": {},
   "source": [
    "## Prepare tokendict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a615b9f0-185f-4de2-a6fe-910cc6c992be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokendict_list = [{\"A\": [], \"G\": [], \"T\": [],\"C\": []} for x in range(6)]\n",
    "\n",
    "for tpl in itertools.product(\"ACGT\",repeat=6):\n",
    "    encoding = tokenizer.encode(\"\".join(tpl))\n",
    "    for idx, nuc in enumerate(tpl):\n",
    "        tokendict_list[idx][nuc].append(encoding[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a7a66-b2bf-4525-b2e5-ecff6ae664f1",
   "metadata": {},
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f00660-95e0-4d15-b7d7-54c349eab162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 6\n",
    "predicted_prbs,gts = [],[]\n",
    "#print (dataset.iloc[0]['seq_chunked'])\n",
    "\n",
    "for no_of_index, tokenized_data in tqdm.tqdm(enumerate(data_loader)):\n",
    "    #if no_of_index < 1340:\n",
    "    #    continue\n",
    "    label = dataset.iloc[no_of_index]['seq_chunked']\n",
    "    label_len = len(label)\n",
    "    #print(no_of_index, label_len)\n",
    "    \n",
    "    # Edge case: for a sequence less then 11 nt\n",
    "    # we cannot even feed 6 mask tokens\n",
    "    # so we might as well predict random\n",
    "    if label_len < 11: \n",
    "        #print (no_of_index)\n",
    "        for i in range(label_len):\n",
    "            predicted_prbs.append(torch.tensor([0.25,0.25,0.25,0.25,0.0]))\n",
    "            gts.append(label[i])\n",
    "        continue\n",
    "\n",
    "        \n",
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    tokenized_data['labels'][tokenized_data['labels']==-100] = 0\n",
    "    inputs = model_input_unaltered.clone()\n",
    "    \n",
    "\n",
    "    # First 5 nucleotides we infer from the first 6-mer\n",
    "    inputs[:, 1:7] = 4 # we mask the first 6 6-mers\n",
    "    inputs = inputs.to(device) \n",
    "    model_pred = torch.softmax(model(inputs)['logits'], dim=2)\n",
    "    for i in range(5):\n",
    "        res,gt = extract_prbs_from_pred(prediction=model_pred[0],\n",
    "                                        pred_pos=1, # first 6-mer (after CLS)\n",
    "                                        token_pos=i, # we go thorugh first 6-mer\n",
    "                                        label_pos=i,\n",
    "                                        label=label)\n",
    "        predicted_prbs.append(res)\n",
    "        gts.append(gt)\n",
    "    \n",
    "\n",
    "\n",
    "    # we do a batched predict to process the rest of the sequence\n",
    "    predictions = predict_on_batch(tokenized_data, dataset, no_of_index)\n",
    "    \n",
    "    # For the 6th nt up to the last 5 \n",
    "    # we extract probabilities similar to how the model was trained\n",
    "    # hiding the 4th nt of the 3rd masked 6-mer of a span of 6 masked 6-mers\n",
    "    # note that CLS makes the tokenized seq one-based\n",
    "    pos = 5 # position in sequence\n",
    "    for pos in range(5, label_len-5):\n",
    "        model_pred = predictions[pos-5]\n",
    "        res,gt = extract_prbs_from_pred(prediction=model_pred,\n",
    "                                        pred_pos=pos-2, # for i-th nt, we look at (i-2)th 6-mer\n",
    "                                        token_pos=3, # look at 4th nt in 6-mer\n",
    "                                        label_pos=pos,\n",
    "                                        label=label)\n",
    "        predicted_prbs.append(res)\n",
    "        gts.append(gt)\n",
    "        \n",
    "    # Infer the last 5 nt from the last 6-mer\n",
    "    for i in range(5):\n",
    "        model_pred = predictions[pos-5]\n",
    "        res,gt = extract_prbs_from_pred(prediction=model_pred,\n",
    "                                pred_pos=pos+1, # len - 5 + 1 = last 6-mer (1-based)\n",
    "                                token_pos=i+1, # we go through last 5 of last 6-mer\n",
    "                                label_pos=pos+i,\n",
    "                                label=label)\n",
    "        predicted_prbs.append(res)\n",
    "        gts.append(gt)\n",
    "\n",
    "    assert(len(gts) == torch.stack(predicted_prbs).shape[0]), \"{} iter, expected len:{} vs actual len:{}\".format(no_of_index,\n",
    "                                                                                   len(gts), \n",
    "                                                                                   torch.stack(predicted_prbs).shape[0])\n",
    "\n",
    "    #XABCDEFGHIJKL -> XABCDE [ABCDEF BCDEFG CDEFGH DEFGHI EFGHIJ FGHIJK] GHIJKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ebe5ac0c-e521-46b6-ad38-a8915a810788",
   "metadata": {},
   "outputs": [],
   "source": [
    "prbs_arr = np.array(torch.stack(predicted_prbs))\n",
    "np.save(\"bertadn_preds_jun.npy\", prbs_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564eb1b9-1249-464a-ad28-4de86b1feb43",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "24320f1c-c55c-4a64-942c-aa9f24307cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49436766"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.max(prbs_arr,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d896b80c-0362-460e-9cfc-013689861d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3954733649623095"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "np.sum(gts == np.array([\"A\",\"C\",\"G\",\"T\"])[np.argmax(prbs_arr,axis=1)])/len(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "25411478-cf13-4ecc-bcaf-bc9591af8094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Actual 0.33590633376540024, Predicted 0.388372575915835\n",
      "C: Actual 0.15707153014610573, Predicted 0.07040855492493361\n",
      "G: Actual 0.14310905704541738, Predicted 0.04891584762719756\n",
      "T: Actual 0.3639130790430766, Predicted 0.49230302153203376\n"
     ]
    }
   ],
   "source": [
    "for nt in [\"A\", \"C\", \"G\", \"T\"]:\n",
    "    nt_arr = np.array([nt]*len(gts))\n",
    "    actual = np.sum(gts == nt_arr)/len(gts)\n",
    "    predicted = np.sum(np.array([\"A\",\"C\",\"G\",\"T\"])[np.argmax(prbs_arr,axis=1)] == nt_arr)/len(gts)\n",
    "    print(\"{}: Actual {}, Predicted {}\".format(nt, actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a42eec2b-ec52-4142-86f2-025b4387953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prbs = torch.log(torch.stack(predicted_prbs)[:,:-1])\n",
    "class_labels = torch.tensor(class_label_gts(gts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fdc7e2e8-b962-4f14-82d4-4d512a7b11cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4430)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.nll_loss(log_prbs, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c2aa84-aa6d-418c-beeb-d64fcbdbe56e",
   "metadata": {},
   "source": [
    "# Make data fit metrics handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "aa217b54-a115-4c44-bd8b-af28858fd4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_path = \"outputs/gpar_bertadn/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5821776e-d73e-48fe-9517-a70d8fc86b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get targets\n",
    "targets = torch.tensor(class_label_gts(gts))\n",
    "stacked_prbs = torch.stack(predicted_prbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5b1ac2f0-f18b-42cd-8ef3-a0f145bd32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cross entropy, it's already as probability so just nll\n",
    "ce = torch.nn.functional.nll_loss(stacked_prbs, targets, reduction=\"none\") #cross_entropy(prbs, targets)\n",
    "\n",
    "#print(ce)\n",
    "\n",
    "# save\n",
    "torch.save(stacked_prbs,  out_path+\"masked_logits.pt\") # no logits, so use prbs\n",
    "torch.save(torch.argmax(stacked_prbs, dim=1),  out_path+\"masked_preds.pt\")\n",
    "torch.save(stacked_prbs,  out_path+\"prbs.pt\")\n",
    "torch.save(ce, out_path+\"ce.pt\")\n",
    "\n",
    "# save targets\n",
    "torch.save(targets, out_path+\"masked_targets.pt\")\n",
    "\n",
    "# save rest as placeholders (zeros of same length)\n",
    "torch.save(torch.zeros(len(stacked_prbs)), out_path+\"masked_motifs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6acad-bbbd-47c1-81bd-4fd9a5e5b7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-dnabert]",
   "language": "python",
   "name": "conda-env-anaconda-dnabert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
